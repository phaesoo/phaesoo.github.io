---
title: "Kong API Gateway #2 - Custom plugin 구현 및 배포하기"
last_modified_at: 2021-03-04T23:00:00-00:00
categories:
  - Blog
tags:
  - apigateway
  - kubernetes
  - kong
---


# Custom plugin 구현 Workflow


---


앞서 1편에서 언급한것과 같이 Kong자체가 Openresty기반으로 구현되었기 때문에 Custom Plugin또한 추가적인 세팅 없이 Lua script로 구현이 가능하다. 

Plugin을 제작하기에 앞서 Plugin제작의 전체적인 workflow에 대하여 설명해보겠다.


# Service architecture


---


API Gateway 도입을 처음으로 검토하게된 계기가 인증관련 모듈을 비즈니스로직 서비스에서 분리해 내기 위함이었고 이미 공식문서에 굉장히 좋은 예제가 있어 구현 부분에서는 많은부분 참고하였다.

아래 그림과 같이 이번에 간단히 만들어볼 Custom plugin은 Upstream API(비즈니스로직 서버)로 가기 이전 단계에서 자체 인증 로직을 구현한 외부 서비스를 통하여 인증을 먼저 수행하도록 routing하는 용도이다.

간단히 Custom plugin의 workflow을 나열하면 다음과 같다.

- Client request에서 인증에 필요한 정보를 추출하여 외부 인증 서비스로 인증 요청을 한다.
- 인증 서비스에서 성공응답을 보낸경우 이후 Upstream API에 요청하는 과정이 수행되도록 한다.
- 인증 서비스에서 실패응답을 보낸 경우 이후 Upstream API로의 요청이 가지 않도록 Kong API에서 실패응답을 client에게 보낸다.

![Image Alt ingress-contoller]({{"/assets/images/kong-for-k8s/service-architecture.png"| relative_url}})

*참고문서: [https://konghq.com/blog/custom-authentication-and-authorization-framework-with-kong/](https://konghq.com/blog/custom-authentication-and-authorization-framework-with-kong/)


# Custom plugin을 만들어보자


---

## 0. 준비단계

### Upstream API(비즈니스로직 서비스) 배포

Custom plugin제작에서 실제 비즈니스 로직을 수행하는 Upstream API까지 구체적으로 다룰 필요는 없지만 실제로 Custom plugin에 의해 응답 성공/실패 처리가 수행 되는지 검증하기 위해서는 간단히 배포할 필요성이 있다. 또한 'Workflow'의 4번째 단계에서 Upstream API의 Ingress에 custom-plugin을 적용하여 설정해야 하기 때문에 간단한 서비스를 배포해 보도록 하겠다. 테스트가 아닌 실제 환경에서는 Upstream API가 먼저 구현되어 있을 가능성이 높으므로 그러한 경우에는 넘어가도 되는 단계이다.

---

**upstream_api.yaml 생성:**

요청에 대하여 간단하게 요청에 대한 정보를 응답으로 보내주는 [ealen/echo-server](https://hub.docker.com/r/ealen/echo-server)를 이용하여 구성해 보았다.

```yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    app: echo
  name: service-echo
spec:
  ports:
  - port: 8080
    protocol: TCP
    targetPort: 80
  selector:
    app: echo
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: echo
  name: echo
spec:
  replicas: 1
  selector:
    matchLabels:
      app: echo
  template:
    metadata:
      labels:
        app: echo
    spec:
      containers:
      - image: ealen/echo-server
        name: echo-server
```

---

**Upstream API 배포:**

```bash
$ kubectl apply -f upstream_api.yaml
```

---

### 간단한 외부 인증 서비스 구현 및 배포

Full code: [phaesoo/testapi](https://github.com/phaesoo/testapi)

아래는 flask로 구현한 간단한 외부 인증 서비스 구현의 서비스 부분 코드이며 아주 간단하게 token과 요청된 urlpath의 vaildation을 통해 성공 혹은 실패를 응답한다. 

해당 서비스의 docker image는 이미 docker hub에 배포되어 kubernetes에서 docker image name으로 pull해 올수 있도록 준비되어 있다. Kubernetes 리소스 정의는 이곳([manifest](https://github.com/phaesoo/testapi/tree/main/k8s))을 참고하면 된다.

인증 로직을 간단히 설명하면 다음과 같다.

- /apikey-simple/public으로의 요청은 인증 토큰 없이 성공응답
- /apikey-simple/private으로의 요청은 인증토큰이 유효하고, 요청된 url path가 유효한 경우 성공응답 이외 401 응답

---

**Custom 인증 로직 구현:**

```python
# In-memory mock databases for users and whitelisted url paths
_users = {
    "token-1": "user-1",
}
_private_paths = ["/private"]
_public_paths = ["/public"]

@ns.route("/verify")
class Verify(Resource):
    def post(self, **kwargs):
        try:
            parsed = parser.parse_args()
        except Exception:
            return resp.error("Invalid request arguments")

        # OK if requested url path is in public path
        if parsed.path in _public_paths:
            return resp.ok(
                {"userUuid": "None"},
                msg="Verified",
            )

        raw_token = request.headers.get("Authorization")
        if raw_token is None:
            return resp.unauthorized()

        if "Bearer " != raw_token[:7]:
            return resp.unauthorized("Token should have to start with 'Bearer '")

        token = raw_token[7:]

        # Check whether requested token is valid or not
        user_uuid = _users.get(token)
        if user_uuid is None:
            return resp.unauthorized("Unknown user")

        # Check whether requested url path is in private path
        if parsed.path not in _private_paths:
            return resp.unauthorized("User has no permission on the path")

        return resp.ok(
            {"userUuid": user_uuid},
            msg="Verified",
        )
```

---

**Custom 인증 서비스 배포:**

project의 root에서 서비스 배포를 위해 다음 명령을 실행한다.

```bash
$ kustomize build k8s | kubectl apply -f -
```

## 1. Custom plugin 구현

자 이제 custom plugin을 실제로 구현해보자.

기본적인 Plugin 작성을 위해서는 다음과 같은 2가지 lua script가 필수적으로 필요하다.

```
custom-plugin
├── handler.lua
└── schema.lua
```

- handler.lua: plugin의 logic을 구현하는 파트. init_worker, access등의 function name처럼 정해진 이름으로 function을 정의할 경우 정의된 시점에 kong에 의해 수행되게 됨.
- schema.lua: plugin의 configuration을 정의. 또한 request의 entity(header, fields)에 대한 validation도 가능함.

Plugin인은 위처럼 간단히 2가지 lua script만 있어도 생성할 수 있지만 실제 제작 및 배포시에는 조금 더 많은 기능들이 필요하다. 예를들면 metadata정의, 테스팅이 있다.

Kong은 고맙게도 [kong-plugin](https://github.com/Kong/kong-plugin)이라는 github project template 를 제공하여 개발자가 좀 더 비즈니스 로직 구현에만 집중할 수 있도록 도움을 준다.

해당 template을 기반으로 custom plugin을 제작하게 되면 kong-pongo를 통해 유닛테스트, 통합테스트가 가능하고 rockspec을 통해 Kong plugin배포도 도움을 받을 수 있다.

위 template을 기반으로 예제를 위한 custom plugin을 작성하였으며 모든 코드는 다음 링크에서 확인할 수 있다. ([phaesoo/kong-plugin-custom-auth](https://github.com/phaesoo/kong-plugin-custom-auth))

---

**schema.lua 정의** ([Full code](https://github.com/phaesoo/kong-plugin-custom-auth/blob/main/kong/plugins/custom-auth/handler.lua)):

schema에서는 간단하게 3가지 필수 필드(auth_host, auth_port, auth_urlpath)를 정의하였으며 1개의 필수 header(Authorization)를 정의하였다. 위 3가지 필드는 이후 KongPlugin 오브젝트 정의(yaml)의 config에 정의될 예정이다.

```lua
local typedefs = require "kong.db.schema.typedefs"

-- Grab pluginname from module name
local plugin_name = ({...})[1]:match("^kong%.plugins%.([^%.]+)")

local schema = {
  name = plugin_name,
  fields = {
    -- the 'fields' array is the top-level entry with fields defined by Kong
    { protocols = typedefs.protocols_http },
    { config = {
        -- The 'config' record is the custom part of the plugin schema
        type = "record",
        fields = {
          {
            auth_host = {
              type = "string",
              required = true,
            },
          },
          {
            auth_port = {
              type = "integer",
              required = true,
            },
          },
          {
            auth_urlpath = {
              type = "string",
              required = true,
            },
          },
          {
            token_header = typedefs.header_name {
              default = "Authorization",
              required = true
            },
          }
        },
        entity_checks = {
        },
      },
    },
  },
}

return schema
```

---


**handler.lua 정의** ([Full code](https://github.com/phaesoo/kong-plugin-custom-auth/blob/main/kong/plugins/custom-auth/handler.lua)):

handler에서는 TokenHandler라는 객체를 생성하였고 access function을 정의하였다. 간단하게 resty.http 모듈을 사용하여 외부 인증 서비스에 인증요청을 하는 로직을 구현하였다.

PRIORITY에 지정된 값의 순서에 따라 kong plugin간 호출 순위가 결정된다. 또한 schema.lua에서 정의된 필드들이 이곳에서 사용되는 것을 확인할 수 있다.

```lua
local http = require "resty.http"
local json = require "cjson"

local TokenHandler = {
  PRIORITY = 1000,
  VERSION = "0.1",
}

function TokenHandler:access(conf)
  kong.log.inspect(conf)

  local jwt_token = kong.request.get_header(conf.token_header)
  if not jwt_token then
    kong.log.debug("Token not found in header")
    kong.response.exit(401)
  end

  local token_type = jwt_token:sub(0,7)
  if token_type ~= "Bearer " then
    kong.log.debug("Invalid token type: ", token_type)
    kong.response.exit(401)
  end
  
  kong.log.debug(conf.auth_host, conf.auth_port)

  local httpc = http.new()
  httpc:connect(conf.auth_host, conf.auth_port)
  
  local res, err = httpc:request({
    method = "POST",
    path = conf.auth_urlpath,
    headers = {
      ["Content-Type"] = "application/json",
      [conf.token_header] = jwt_token
    },
    body = json.encode({
      path = kong.request.get_path(),
      raw_query = kong.request.get_raw_query(),
    }),
  })

  if not res then
    kong.log.err("Failed to call auth_endpoint:", err)
    return kong.response.exit(500)
  end

  if res.status ~= 200 then
    kong.log.debug("Authentication failed", res.status)
    return kong.response.exit(401) -- unauthorized
  end
end

return TokenHandler
```

---


**Tips**

번외로 작성된 Custom plugin은 kong-pongo를 통해 쉽게 테스트를 작성할 수 있으며 test code는 project하위의 spec에 모아놓는것이 일반적이다.

```lua
pongo run spec/{target_dir}
```

## 2. Configmap or Secret 생성

Custom plugin은 Kong을 운영하는 모드에 따라 그 방법이 상이한데. DB-less모드로 운영하는 경우 configmap이나 secret의 형태로 선언적으로 정의되고 관리될 수 있다.

---

**confimap/secret 생성:**

namespace는 kong, plugin name, directory구조는 위에서 언급한 phaesoo/kong-plugin-custom-auth를 따른다고 가정한다. project의 root directory 에서 명령을 수행하였다. 실제 환경에서는 둘중 한가지만 수행하면 되며 둘의 차이점은 리소스의 암호화 여부이다. 내부에 유출되면 안되는 비즈니스 로직이나 configuration을 담고 있으면 secret으로 리소스를 생성하면 될 것으로 보인다.

```bash
# create configmap
$ kubectl -n kong create configmap kong-plugin-custom-auth --from-file=kong/plugins/custom-auth
# or create secret
$ kubectl -n kong create secret generic kong-plugin-custom-auth --from-file=kong/plugins/custom-auth
```

## 3. Patch Kong ingress controller manifest

위에서 생서된 configmap(or secret) 리소스를 kong의 deployment에 실제 반영하기 위하여 configmap(or secret)의 volume을 mount해야 하는데 기존 kong-ingress-controller의 deployment 정의에 반영하는 전략은 여러가지 방법이 있을수 있다. 그 중 이번에는 간단하게 기존 배포에 업데이트 사항을 patch하는 방향으로 진행해 보겠다. (kustomize의 patchesStrategicMerge)

작업하는 directory 구조는 다음과 같다.

```
manifest
├── patch_deployment.yaml
└── kustomization.yaml
```

---

**patch_deployment.yaml:**

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ingress-kong
  namespace: kong
spec:
  template:
    spec:
      containers:
      - name: proxy
        env:
        - name: KONG_PLUGINS
          value: bundled,custom-auth
        - name: KONG_LUA_PACKAGE_PATH
          value: "/opt/?.lua;;"
        - name: KONG_LOG_LEVEL
          value: debug
        volumeMounts:
        - name: kong-plugin-custom-auth
          mountPath: /opt/kong/plugins/custom-auth
      volumes:
      - name: kong-plugin-custom-auth
        configMap:
          name: kong-plugin-custom-auth
```

---

**kustomization.yaml:**

```yaml
resources:
- https://github.com/kong/kubernetes-ingress-controller/deploy/manifests/base
patchesStrategicMerge:
- custom-plugin.yaml
```

---

**Patch 적용 및 확인:**

```bash
# patch 적용
$ kustomize build manifest | kubectl apply -f -
# patch 결과 상세 확인
$ kubectl -n kong describe deployments.app ingress-kong
```

## 4. Upstream API의 Ingress정의

custom-plugin을 kong에 추가만 했다고 해서 모든 Ingress에 적용되는 것은 아니다. 실제 custom plugin의 사용 여부는 Ingress별로 정의되게 되며 이는 각각의 서비스들이 어떤 plugin을 사용할지를 결정할 수 있다는 것을 의미하며 합리적이라고 생각된다.

'0. 준비단계' 에서 배포한 Upstream API에 대하여 Ingress를 정의하여 cluster외부에서 오는 요청에 대하여 routing규칙을 생성해 보도록 하겠다. 물론 custom-plugin을 ingress정의에 포함시켜 요청단계에서 인증을 수행하도록 한다.

---

**ingress_echo.yaml:**

```yaml
apiVersion: configuration.konghq.com/v1
kind: KongPlugin
metadata:
  name: custom-auth
config:
  auth_host: "testapi.default"  # default namespace의 testapi serveice
  auth_port: 8080
  auth_urlpath: "/apikey-simple/verify"
plugin: custom-auth
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ingress-echo
  annotations:
    kubernetes.io/ingress.class: kong  # ingress controller로 kong을 지정한다.
    konghq.com/plugins: custom-auth  # plugin이 추가될 경우 comma seperate로 구분하여 추가한다.
		plugins.konghq.com: custom-auth
spec:
  rules:
  - http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: service-echo
            port:
              number: 8080
```


# 외부 요청 테스트


---


자 이제 모든 리소스에 대한 배포가 완료되었고 마지막으로 검증하는 단계가 남았다.

외부에서 Kong proxy를 통해 요청을 보내는 테스트를 수행해 봄으로서 정상적으로 custom plugin을 통한 인증이 수행되는지 확인할 수 있다.

테스트 환경에서 minikube 기반으로 테스트 하는 경우 먼저 minikube tunnel을 통해 cluster의 ip를 host로 노출시켜 주어야 한다.

---

**(optional) minikube tunnel:**

tmux등의 화면 분할 도구로 수행하여 테스트 종료시 까지 process를 유지시킨다.

```bash
$ minikube tunnel
```

---

**kong-proxy의 EXTERNAL-IP 확인 및 환경변수 설정:**

```bash
$ kubectl -n kong get svc kong-proxy
NAME                      TYPE           CLUSTER-IP      EXTERNAL-IP     PORT(S)                      AGE
kong-proxy                LoadBalancer   10.105.58.220   10.105.58.220   80:30229/TCP,443:31688/TCP   22d

$ export PROXY_IP=10.105.58.220
```

---

**시나리오 테스트:**

간략히 다음 네가지 시나리오에 대하여 진행하여 custom plugin이 정상적으로 동작 하는지 확인할 수 있다.

```bash
# case 1) 인증 토큰 없이 public endpoint 요청 → 인증성공 후 echo서버 응답 수신
$ curl ${PROXY_IP}/apikey-simple/public
# 성공응답

# case 2) valid 토큰 으로 private endpoint요청 → 인증성공 후 echo서버 응답 수신
$ curl ${PROXY_IP}/apikey-simple/private \
-H "Authorization: Bearer token-1"

# case 3) invalid 토큰 으로 private endpoint요청 → 인증실패로 401응답
$ curl ${PROXY_IP}/apikey-simple/private \
-H "Authorization: Bearer token-2"

# case 4) valid 토큰 으로 invalid endpoint요청 → 인증실패로 401응답
$ curl ${PROXY_IP}/apikey-simple/dummy \
-H "Authorization: Bearer token-1"
```

---

**Troubleshooting:**

간혹 응답이 다음과 같이 올 경우 ingress-controller의 log를 확인하여 원인을 파악할 수 있다. 주로 config 관련 문제인 경우가 많았다.

```bash
$ curl ${PROXY_IP}/apikey-simple/public
{“message”:“no Route matched with those values”}

$ k -n kong logs ${ingress-kong의 pod name} ingress-controller
```
